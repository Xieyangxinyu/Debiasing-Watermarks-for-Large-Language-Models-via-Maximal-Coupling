# Unbiased Watermark via Importance Sampling

This repository reproduces the results reported in [Unbiased Watermark via Importance Sampling]().

## Requirements
First, install [PyTorch](https://pytorch.org/get-started/locally/). The remaining dependency can be installed using the following command:
```
pip install -r requirements.txt
```
This codebase was adapted from [three_bricks](https://github.com/facebookresearch/three_bricks) and developed with python version 3.11, PyTorch version 2.1.1, CUDA 12.0. However, we do not support multi-bit watermarking in our code.

## Models

We use two models: [OPT-1.3B](https://huggingface.co/facebook/opt-1.3b) and [Sheared-LLaMA-2.7B](https://huggingface.co/princeton-nlp/Sheared-LLaMA-2.7B). In all our codes, `{model_name}` are either `opt` or `llama`, referring to these two models.

## Data
We adapted two long-form question answering datasets from [WaterBench](https://arxiv.org/abs/2311.07138). The data files are under `data/`.
The original responses from the models are generated by running `original_response.py`, for example,
```
python original_response.py --model_name llama --data finance_qa --batch_size 32
``` 
and saved in files of the form `{dataset}_qa_{model_name}.jsonl`, for example, `finance_qa_llama.jsonl`. Then, you can run `convert_data.py` to convert this `jsonl` file into a `json` file, for example,
```
python convert_data.py --prompt_path data/longform_qa
```

## Usage
You can reproduce the results of the paper by the following steps:
1. Run the `main_watermark.py` file with the appropriate command-line arguments.
<details>
<summary><span style="font-weight: bold;">Command Line Arguments for main_watermark.py</span></summary>
    
- `--model_name`: The name of the pre-trained model to use for text generation and analysis. Supported model names include "opt" and "llama".
- `--prompt_path`: The path to the JSON file containing prompts. Default value: "data/alpaca_data.json."
- `--method`: Choose a watermarking method for text generation. Options: "none" (no watermarking), "openai" (Aaronson et al.), "maryland" (Kirchenbauer et al.), "importance" which is our method. Default value: "none."
- `--method_detect`: Choose a statistical test to detect watermark. "same" uses the grounded statistical test with the same method as for generation. The other options are "openai", "maryland", "importance-max", "importance-sum". Default value: "same." For our proposed method, please use "importance-sum".
- `--one_list`: Use only a single green list; only works if detection method is importance-sum. See details in the Appendix of the paper.
- `--scoring_method`: Method for scoring tokens. Options: "none" (score every token), "v1" (score token when the watermark context is unique), "v2" (score token when {wm context + token} is unique). Default value: "none." We use "v2" for all our experiments.
- `--ngram`: Watermark context width for RNG key generation. Default value: 4. We use either 1 or 4 for all our experiments.
- `--gamma`: Size of the green lists.

</details>

Here is an example:
```cmd
python main_watermark.py --model_name opt \
    --prompt_path data/finance_qa_opt.json --one_list \
    --method importance --method_detect importance-sum \
    --output_dir output/finance_qa/opt/importance_one_list/gamma_0.5_ngram_1 \
    --gamma 0.5 --ngram 1 --scoring_method v2 --batch_size 32
```

2. The previous script generates watermarked text and saves the results in the specified output directory. The output files include:

**`results.jsonl`.** Contains the generated watermarked text for each prompt in JSONL format. 
**`scores.jsonl`.** Contains the analysis results for each watermarked text in JSONL format.
In particular, the `scores.jsonl` file contains the following fields:

| Field | Description |
| --- | --- |
| `text_index` | Index of the prompt in the JSON file |
| `num_token` | Number of analyzed tokens in the text |
| `score` | Watermark score of the text |
| `pvalue` | p-value of the detection test |
| `score_sbert` | Cosine similarity score between watermarked completion and ground truth answer |

For `importance` methods, there are two additional fields:
| Field | Description |
| --- | --- |
| `entropy` | The average entropy of the text |
| `percentage_less_than_095` | The percent of $p^t(G)$'s below 0.95  |

3. To compute the true positive rates (TPR), run `main_eval.py`. For example,
```
python main_eval.py --tokenizer opt --json_path output/longform_qa/opt/importance_one_list/gamma_0.5_ngram_1 \
    --text_key result --method importance-sum \
    --gamma 0.5 --seeding hash --ngram 1 --scoring_method v2 --alpha 0.01
```

4. To compute the true positive rates (TPR) after substitution attacks, run `main_eval.py` with the appropriate command-line arguments. For example,
```
python main_eval.py --tokenizer opt --json_path output/longform_qa/opt/importance_one_list/gamma_0.5_ngram_1 \
    --text_key result --attack_name tok_substitution --method importance-sum \
    --gamma 0.5 --seeding hash --ngram 1 --scoring_method v2 --alpha 0.01
```

<details>
<summary><span style="font-weight: bold;">Typically, you will pay attention to these arguments:</span></summary>
    
- `--tokenizer`: The name of the tokenizer model to use. Supported model names include "opt" and "llama".
- `--json_path`: The path to the folder containing the `results.jsonl` file. 
- `--text_key`: We use "result" for all our experiments.
- `--attack_name`: We use use "tok_substitution" for all our experiments.
- `--method`: Choose a statistical test to detect watermark. "same" uses the grounded statistical test with the same method as for generation. The other options are "openai", "maryland", "importance-max", "importance-sum", "importance-HC". For our proposed method, please use "importance-sum".
- `--gamma`: Size of the green lists.
- `--seeding`: We use "hash" for all our experiments.
- `--ngram`: Watermark context width for RNG key generation. Default value: 4. We use either 1 or 4 for all our experiments.
- `--scoring_method`: We use "v2" for all our experiments.
- `--alpha`: The significance level used for the test. This is not supported for "importance-HC". See details for the higher criticism statistics below.
</details>

5. Since the decision boundary for higher criticism statistics is obtained from simulation, please check `HC_simulate.py` for pre-computing the decision boundaries at a specified significance level.

6. The code for simulation studies are included in `regime_1.py` and `regime_2.py`, corresponding to the two scenarios discussed in the paper.